\documentclass[12pt, twoside]{article}
\usepackage{jmlda}
\usepackage{tikz}
\usetikzlibrary{matrix}

\usepackage[pdftex,unicode, 
colorlinks=true,
linkcolor = blue
]{hyperref}	% нумерование страниц, ссылки!!!!ИМЕННО В ТАКОМ ПОРЯДКЕ СО СЛЕДУЮЩИМ ПАКЕТОМ
\newcommand{\hdir}{.}
%\linespread{2} %Потом убрать
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bY}{\mathbf{Y}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bP}{\mathbf{P}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bQ}{\mathbf{Q}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\bE}{\mathbf{E}}
\newcommand{\bF}{\mathbf{F}}
\newcommand{\bU}{\mathbf{U}}
\newcommand{\bW}{\mathbf{W}}

\usepackage{graphicx}

\graphicspath{{./figures/}}
\usepackage{subcaption}
\usepackage{neuralnetwork}

\begin{document}

\title
    [Исследование способов согласования моделей с помощью снижения размерности пространства] % краткое название; не нужно, если полное название влезает в~колонтитул
    {Исследование способов согласования моделей с помощью снижения размерности пространства}
\author
    [Ф.\,Р.~Яушев, Р.\,В.~Исаченко, В.\,В.~Стрижов] % список авторов (не более трех) для колонтитула; не нужен, если основной список влезает в колонтитул
    {Ф.\,Р.~Яушев, Р.\,В.~Исаченко, В.\,В.~Стрижов} % основной список авторов, выводимый в оглавление
    [Ф.\,Р.~Яушев, Р.\,В.~Исаченко, В.\,В.~Стрижов] % список авторов, выводимый в заголовок; не нужен, если он не отличается от основного
\email
    {yaushev@phystech.edu; roman.isachenko@phystech.edu; strijov@ccas.ru}

\abstract
    {В работе исследуется задача прогнозирования сложной целевой переменной. Под сложностью подразумевается наличие зависимостей (линейных или нелинейных). Предполагается, что исходные данные гетерогенны. Это значит, что пространства независимой и целевой переменных имеют разную природу. Предлагается построить предсказательную модель, которая учитывает зависимость в исходном пространстве независимой переменной, а также в пространстве целевой переменной. Согласование моделей предлагается производить в низкоразмерном пространстве. В качестве базового алгоритма используется метод проекции в скрытое пространство (PLS). В работе проводится сравнение линейного PLS и предложенных нелинейных моделей. Сравнение производится на гетерогенных данных в пространствах высокой размерности.
	
\bigskip
\noindent
\textbf{Ключевые слова}: \emph {}
}



%данные поля заполняются редакцией журнала
%\doi{10.21469/22233792}
%\receivedRus{25.02.2020}
%\receivedEng{February 25, 2020}


\maketitle
%\linenumbers

\section{Введение}
В данной работе решается задача прогнозирования целевой переменной с наличием зависимостей. Трудность задачи в том, что исходные данные имеют высокую размерности и в пространствах целевой и независимой переменных есть скрытые зависимости. Чрезмерно высокая размерность пространств и наблюдаемая множественная корреляция приводят к неустойчивости модели. Для решения предлагается построить модель, которая бы учитывала обе эти зависимости. Она переводит данные в низкоразмерные пространства и согласование данных происходит в полученном скрытом пространстве.

Метод проекции в скрытое пространство (Projection to Latent Space, PLS) \cite{overview_pls, overview_nonlinear_pls} восстанавливает зависимости между двумя наборами данных. Он применяется в биоинформатике, медицине, социальных науках \cite{1figures, btc519, PLS_in_strategic_management, PLS_application}. Алгоритм PLS строит матрицу совместного описания признаков и целевой переменной. Полученное пространство является низкоразмерным. Это позволяет получить простую, точную и устойчивую прогностическую модель. Наряду с PLS используется алгоритм CCA \cite{cca_alg}. ССА применяется для поиска зависимостей между двумя наборами данных и получения их низкоразмерного представления \cite{cca_apl1, cca_apl2}. CCA максимизирует корреляции, а PLS~--- ковариации. Обзор и сравнение CCA и PLS приводится в \cite{overview_pls}. PLS и CCA~--- линейные модели, которые игнорируют сложные нелинейные зависимости. 

Задачи, в которых между данными существует нелинейная зависимость описаны в работе~\cite{?}. Аппроксимация этой зависимость линейной PLS моделью приводит к неудовлетворительным результатам. Разработано нелинейные модификации PLS \cite{PLS_nn, PLS_rbf, PLS_ga} и ССA \cite{deep_cca, kernel_cca}. Например, Deep CCA \cite{deep_cca} преобразует исходные данные с помощью нейронной сети таким образом, что результирующее представление становится согласованным. Deep CCA используется для генерации текстового описания по изображениям в работе \cite{kernel_cca_appl}. 

% В это работе исследуется сложность прогностических моделей. Предлагается...

В работе проведено два эксперимента. Первый эксперимент направлен на сравнение эффективности Deep CCA и CCA на задаче классификации зашумленных цифровых изображений MNIST \cite{MNIST}. Во втором эксперименте используется набор данных, полученный делением каждого изображения из MNIST на левую и правую части. На задаче регрессии правой части изображения по левой проводится сравнение нелинейных моделей с применением автоэнкодеров, моделей без преобразования данных и линейного PLS. На основании полученных результатов сделан вывод о точности и сложности нелинейных алгоритмов и о целесообразности использования той или иной модели.

\section{Постановка задачи}

Пусть дана выборка $(\bX, \bY)$, где $\textbf{X} = [\textbf{x}_1, \dots, \textbf{x}_{n}]^{\T} \in \mathbb{R}^{n \times m}$~--- матрица независимых переменных, $\textbf{Y} = [\textbf{y}_1, \dots, \textbf{y}_n]^{\T} \in \mathbb{R}^{n \times k}$~--- матрица целевых переменных.

\noindent Предполагается, что между $\bX$ и $\bY$ существует зависимость
\begin{equation}
\bY = f(\bX) + \boldsymbol{\varepsilon},
\label{eq:reg}
\end{equation}
где $f: \mathbb{R}^{n \times m} \to \mathbb{R}^{n \times k}$~--- функция регрессионной зависимости, $\boldsymbol{\varepsilon}$~--- матрица регрессионных ошибок.

Необходимо восстановить зависимость $f$ по заданной выборке.

\subsection{Линейная регрессия}
Предположим, что зависимость~\eqref{eq:reg} линейна. Требуется найти эту зависимость:
\begin{equation}
\textbf{\bY} = f(\bX) + \boldsymbol{\varepsilon}=\textbf{\bX}\textbf{W}^{\T} + \boldsymbol{\varepsilon},
\label{eq:model}
\end{equation}
\noindent где $\textbf{W} \in \mathbb{R}^{k \times m}$~-- матрица параметров модели.

\noindent Оптимальные параметры определяются минимизацией функции потерь. Используется квадратичная функция потерь:
\begin{equation}
\mathcal{L}({\bW} | {\bX}, {\bY}) = {\left\| \underset{n \times k}{\bY}  - \underset{n \times m}{\bX} \cdot \underset{m \times k}{\bW}^{\T} \right\| }_2^2 \rightarrow\min_{\bW}.
\label{eq:loss_function_2}
\end{equation}

\noindent Решение~\eqref{eq:loss_function_2} имеет следующий вид:
\begin{equation*}
\bW = \bY^{\T} \bX (\bX^{\T} \bX)^{-1}.
\end{equation*}

Линейная зависимость столбцов матрицы $\bX$ приводит к неустойчивости решения задачи минимизации~\eqref{eq:loss_function_2}, так как в этом случае матрица ${\bX}^{\T} \bX$ является плохо обусловленной.

\noindent Для борьбы с линейной зависимостью используются методы снижения размерности, путем перехода в низкоразмерное латентное пространство.

\begin{Definition}
    Параметрическая функция $\varphi_1: \mathbb{R}^{n \times m} \to \mathbb{R}^{n \times p}$, переводящая исходных данных в латентное пространство, называется \textbf{функцией кодирования}.
\end{Definition}

\begin{Definition}
    Функция $\varphi_2: \mathbb{R}^{n \times k} \to \mathbb{R}^{n \times p}$, переводящая данные из латентного пространства в исходное, называется \textbf{функцией восстановления}.
\end{Definition}

\begin{Definition}
    Функция $g: \mathbb{R}^{n \times p}\times \mathbb{R}^{n \times p} \to \mathbb{R}$, связывающая закономерности в низкоразмерных латентных представления, называется \textbf{функцией согласования}.
\end{Definition}

\begin{Definition}
    Согласование~--- процесс максимизации функции согласования.
\end{Definition}

\subsection{Снижение размерности}

Общая схема модели выглядит следующим образом:
\begin{equation}
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes,row sep=3em,column sep=4em,minimum width=2em]
  {
     \underset{n \times m}{\bX} & \underset{n \times k}{\bY} \\
     \underset{n \times p}{\mathbf{T}} &  \underset{n \times p}{\mathbf{U}} \\};
  \path[-stealth]
    (m-2-1) edge node [right] {$\varphi_2$} (m-1-1)
    (m-2-2) edge node [left] {$\psi_2$} (m-1-2)
    (m-1-1) edge [bend right] node [left] {$\varphi_1$} (m-2-1)
    (m-1-2) edge [bend left] node [right] {$\psi_1$} (m-2-2)
    (m-2-1) edge [<->] node [above] {$g$} (m-2-2)
    (m-1-1) edge [->] node [above] {$f$} (m-1-2)
\end{tikzpicture}
\label{eq:scheme}
\end{equation}

\noindent где $\varphi_1: \mathbb{R}^{n \times m} \to \mathbb{R}^{n \times p}$~---  функция кодирования независимых переменных; $\psi_1: \mathbb{R}^{n \times k} \to \mathbb{R}^{n \times p}$~---  функция кодирования целевых переменных; $\varphi_2: \mathbb{R}^{n \times p} \to \mathbb{R}^{n \times m}$~---  функция восстановления независимых переменных; $\psi_2: \mathbb{R}^{n \times p} \to \mathbb{R}^{n \times k}$ -  функция восстановления целевых переменных; $g: \mathbb{R}^{n \times p} \times \mathbb{R}^{n \times p} \to \mathbb{R}$~--- функция согласования.

\noindent $\bT = \varphi_1(\bX)  \in \mathbb{R}^{n\times p}$ и $\bU =\psi_1(\bY) \in \mathbb{R}^{n\times p}$~--- матрицы представлений данных в латентном пространстве низкой размерности.

Оптимальные параметры $\theta_{\varphi_1}^{*}, \theta_{\psi_1}^{*}$ для функций кодирования $\varphi_1$  и $\psi_1$ находятся из следующей задачи параметрической оптимизации:
\begin{equation}
(\theta_{\varphi_1}^{*}, \theta_{\psi_1}^{*}) = \argmax_{(\theta_{\varphi_1}, \theta_{\psi_1})} [g\bigl( \varphi_1(\bX; \theta_{\varphi_1}), \psi_1(\bY; \theta_{\psi_1})\bigr)].
\label{eq:argmax}
\end{equation}

Так как параметры функции кодирования подбираются из условия максимизации функции согласования~\eqref{eq:argmax}, то после перехода в латентное пространство между $\mathbf{T}$ и $\mathbf{U}$ существует зависимость
\begin{equation}
\bU = h(\bT) +  \boldsymbol{\eta},
\label{eq:reg2}
\end{equation}
где $h: \mathbb{R}^{n \times p} \to \mathbb{R}^{n \times p}$~--- функция регрессионной зависимости,  \boldsymbol{\eta}~--- матрица регрессивных ошибок.

\noindent Оптимальная $h$ выбирается минимизацией функции ошибки. Используем квадратичную функцию ошибки потерь $\mathcal{L}$ на $\bT$ и $\bU$:
\begin{equation}
\mathcal{L}(h | {\bT}, {\bU}) = {\left\| \underset{n \times p}{\bU}  - h(\underset{m \times p}{\bT}) \right\| }_2^2 \rightarrow\min_{h}.
\label{eq:loss_function}
\end{equation}

\noindent Финальная прогностическая модель имеет вид:
 $\widehat{\by} = \psi_2\bigl(h(\varphi_1(\bx))\bigr)$, то есть
 
\begin{equation}
f = \psi_2 \circ h \circ \varphi_1.
\label{eq:f}
\end{equation}

\subsection{Метод Главных Компонент (PCA)}

PCA~--- способ снижения размерности данных, сохраняющий максимальную дисперсию. PCA представляет собой ортогональное линейное преобразование исходного признакового пространства в новое пространство меньшей размерности. Первый базисные векторы строятся так, чтобы выборочная дисперсия данных вдоль них была максимальной:
\begin{equation}
\bp = \argmax_{\|\bp\|_{2} = 1} [\textbf{var}(\bX \textbf{p})],
\label{eq:PCA}
\end{equation}
где $\textbf{var}(\bX \textbf{p}) = \frac{1}{n} (\bX \textbf{p})^{\T}\bX \textbf{p}$ обозначает выборочную дисперсию.

Функция кодирования $\varphi_1: \mathbb{R}^{n \times m} \to \mathbb{R}^{n \times p}$ имеет вид:
\begin{equation}
\varphi_1(\bX) =  \underset{n \times m}\bX \underset{m \times p}\bP^{\T},
\label{eq:PCA2}
\end{equation}
где $\textbf{P} = [\textbf{p}_1, \dots, \textbf{p}_{p}].$

\noindent PCA не согласует независимые переменные и целевые переменные. Из-за этого зависимости в обоих пространствах не учитываются.


\subsection{PLS}

PLS~--- алгоритм для восстановления связи между двумя наборами данных $\bX$ и $\bY$. Алгоритм проецирует $\bX$ и $\bY$ на латентное пространство $\mathbb{R}^{p}$ меньшей размерности. PLS находит матрицы исходных данных $\bX$ и $\bY$ в латентном пространстве $\textbf{T}$ и $\textbf{U}$ соответственно. Матрица объектов $\bX$ и целевая матрица $\bY$ проецируются на латентное пространство следующим образом:
\begin{equation}
\underset{n \times m}{\bX}  = \underset{n \times p}{\textbf{T}} \cdot \underset{p \times m}{\textbf{P}}^{\T} +  \underset{n \times m}{\textbf{F}},
\label{eq:PLSpr1}
\end{equation}
\begin{equation}
\underset{n \times k}{\bY}  = \underset{n \times p}{\textbf{U}} \cdot \underset{p \times k}{\bQ}^{\T} + \underset{n \times k}{\textbf{E}},
\label{eq:PLSpr2}
\end{equation}
\noindent где $\textbf{T}$ и $\textbf{U}$~--- матрицы описания объектов и исходов в латентном пространстве; $\textbf{P}$ и $\textbf{Q}$~--- матрицы перехода из латентного пространства в исходное; $\textbf{F}$, $\textbf{E}$~--- матрицы остатков.

Для PLS  функции кодирования имеют вид:
\begin{equation}
\varphi_1(\bX) = \bX \bW_{\bx}, \;\;
\psi_1(\bY) = \bY \bW_{\by},
\end{equation} 
где матрицы весов $\bW_{\bx} \in \mathbb{R}^{m \times p}, \bW_{\by} \in \mathbb{R}^{k \times p}$ находятся путем максимизации функции согласования $g(\bX \bW_{\bx},  \bY \bW_{\by}) = \textbf{Cov} (\bX \bW_{\bx},  \bY \bW_{\by})^{2}$:
\begin{equation}
(\bW_{\bx}, \bW_{\by}) = \argmax_{\bW_{\by}, \bW_{\by}}[ \textbf{Cov}(\bX \bW_{\bx}, \bY \bW_{\by})^{2}],
\label{eq:PLSpr3}
\end{equation}
где $\textbf{Cov}(\bX \bW_{\bx}, \bY \bW_{\by})$~--- выборочная ковариация.

\noindent Функции восстановления принимают вид:
\begin{equation}
\varphi_2(\bT) = \bT\bP^{\T}, \;\;
\psi_2(\bU) = \bU \bQ^{\T}.
\end{equation} 

\subsection{CCA}

Канонический анализ корреляций (CCA) находит два набора базисных векторов $\{\bw_{\bx}_{i}\}_{i=1}^{p}, \; \bw_{\bx} \in \mathbb{R}^{m}$ и $\{\bw_{\by}_{i}\}_{i=1}^{p}, \; \bw_{\by} \in \mathbb{R}^{k}$, один для $\bX$ и другой для $\bY$, так что коэффициент корреляция между проекциями переменных на эти базисные векторы была максимальной. Функция согласования для CCA
\begin{equation}
g(\bX \bW_{\bx}, \bY \bW_{\by}) = \textbf{corr}(\bX \bW_{\bx}, \bY \bW_{\by}),
\end{equation} 
\noindentгде $\textbf{corr}(\bX \bw_{\bx}, \bY \bw_{\by})$~-- коэффициент корреляции между векторами.

\noindent Таким образом, функции кодирования
\begin{equation}
\varphi_1(\bX) = \bX \bW_{\bx} , \;\;
\psi_1(\bY) = \bY \bW_{\by},
\end{equation}
где первые столбцы матриц весов находится, как вектора максимизирующие функцию согласования $g$. Далее ищутся вектора, максимизирующие $g$, но с ограничением, что они не коррелируют с первой парой векторов. Процедура продолжается до тех пор, пока количество векторов не станет равным $p$. 
\subsection{Deep CCA}

Deep CCA~--- нелинейная модификация CCA. Deep CCA преобразует исходные данные с помощью нейронной сети таким образом, что результирующее представление становится согласованным. Предполагается, что есть $d$ слоев нейронной сети. 

Обозначим $\theta_{\bx}$, $\theta_{\by}$~-- параметры для функций кодирования, то есть матрицы весов и векторы смещений. Оптимальные параметры $\theta_{\bx}^{*}$, $\theta_{\by}^{*}$ находятся из задачи оптимизации:
\begin{equation}
(\theta_{\bx}^{*}, \theta_{\by}^{*}) = \argmax _{(\theta_{\bx}, \theta_{\by})} [g(\varphi_1(\bX; \theta_{\bx}), \psi_1(\bY; \theta_{\by}))] = \argmax _{(\theta_{\bx}, \theta_{\by})} [\textbf{corr}(\varphi_1(\bX; \theta_{\bx}), \psi_1(\bY; \theta_{\by}))].
\end{equation}

\begin{figure}[h!]
    \begin{center}
        \begin{tabular}{m{.3\textwidth}   m{.3\textwidth}}
            \includegraphics[scale=1.5]{figures/noisy_mnist/data1.pdf} & 
            \includegraphics[scale=1.5]{figures/noisy_mnist/data2.pdf} \\
        \end{tabular}
    \end{center}
	\caption{Зашумленные изображений из набора данных MNIST}
	\label{fgr:1}
\end{figure}

\begin{table}[h!]
	\caption{Получение нового признакового пространство размерности 15 с использованием DCCA и CCA. Показателем эффективности будет точность классификации линейного SVM  (ACC).}
	\centering
	\begin{tabular}{l|cc}
		\hline
		& DeepCCA(L=3) & CCA \\  \hline
		Validation data & $92.74\%$  &  $76.21\%$\\
		Test data & $92.14\%$ & $76.07\%$ \\
		\hline
	\end{tabular}
	\label{tbl:1}
\end{table}


\subsection{Эксперимент №1}

Проведем сравнение качества DeepCCA и CCA на задаче классификации зашумленных цифровых изображений~\eqref{fgr:1}. Для этого используется набор данных MNIST \cite{MNIST}, который состоит из 70000 цифровых изображений $28 \times 28$ образцов рукописного написания цифр. Предлагается получить два новых набора данных $\bX$ и $\bY$ следующим образом. Первый набор получим поворотом исходных изображений на угол в диапазоне $[\frac{-\pi}{4}, \frac{\pi}{4}]$. Для получения второго набора данных для каждой картинки из первого набора данных ставится в соответствие случайным образом картинка с той же цифрой, но с добавлением независимого случайного шума, распределенного равномерно на отрезке $[0,1]$.

Применив к двум новым наборам данных DeepCCA или CCA, мы получаем новое низкоразмерное признаковое пространство, которое игнорирует шумы в исходных данных. Таким образом, получаем функции кодирования $\varphi_1$ и $\psi_1$ для исходных наборов данных. На новых признаках, полученных разными моделями (DeepCCA и CCA), для первого набора данных, то есть на данных после применения функции кодирования $\varphi_1$ к первому набору исходных данных, обучим линейный SVM классификатор. Показателем эффективности будет доля правильных ответов SVM на новых данных~--- accuracy (ACC). Результаты эксперимента приведены в таблице ~\eqref{tbl:1}.


\begin{figure}[h!]
\begin{center}
    \begin{tabular}{
    m{.042\textwidth}   m{.05\textwidth}   m{.042\textwidth}   m{.05\textwidth}  m{.042\textwidth}   m{.05\textwidth}   m{.042\textwidth}   m{.05\textwidth}   m{.042\textwidth}   m{.05\textwidth}   
    }

        \includegraphics[scale=3]{figures/original/0.pdf} & 
        \includegraphics[scale=3]{figures/original/1.pdf} & 
        \includegraphics[scale=3]{figures/original/2.pdf} & 
        \includegraphics[scale=3]{figures/original/3.pdf} & 
        \includegraphics[scale=3]{figures/original/4.pdf} & 
        \includegraphics[scale=3]{figures/original/5.pdf} & 
        \includegraphics[scale=3]{figures/original/6.pdf} & 
        \includegraphics[scale=3]{figures/original/7.pdf} & 
        \includegraphics[scale=3]{figures/original/8.pdf} & 
        \includegraphics[scale=3]{figures/original/9.pdf} \\
    
    \end{tabular}
    \end{center}
	\caption{Набор данных MNIST, каждое изображение в котором разделили пополам.}
	\label{fgr:3}
\end{figure}


\begin{table}[h!]
	\caption{Восстановление правой части изображения по левой с использованием различных моделей. Для измерения качества моделей считается среднеквадратическое отклонения от оригинального изображения.
	}
	\centering
	\begin{tabular}{l|cccccc}
		\hline
	   & EncNet1 & LinNet1 & EncNet2 & LinNet2 & DumbNet & PLS\\  \hline
		Кол-во весов & $283$k & $239$k & $283$k & $239$k  & $283$k & -\\ 
		MSE loss on test data & $0.147$ & $ 0.235$ & $0.149$ & $0.236$ & $0.128$ & $0.188$ \\ 
		\hline
	\end{tabular}
	\label{tbl:2}
\end{table}

\subsection{Эксперимент №2}

Проведем на задаче регрессии сравнение нескольких моделей, которые используют автоэнкодеры для снижения размерности пространства, моделей без преобразования исходных данных и линейный PLS. Для этого используется набор данных MNIST. Каждое изображение делится на левую и правую части~\eqref{fgr:3}. Модели по левому изображению предсказывают правое~\eqref{fgr:2}.

Модель EncNet1~--- нейронная сеть с нелинейными функциями активации, которая обучается на данных после преобразования их автоэнкодером. Модель LinNet1~--- нейронная сеть с одним линейным слоем, которая также обучается на преобразованных данных. Для EncNet1 и LinNet1 автоэнкодеры для объектов и ответов используют совместную функцию потерь, которая связывает выходы енкодеров. Модели EncNet2 и LinNet2 устроены аналогично EncNet1 и LinNet1 соответственно, но в автоэнкодерах нет совместной функции потерь. Модель DumbNet~---  нейронная сеть, которая обучается на исходных данных и имеет такую же структуру, что и EncNet, то есть имеет такое же количество слоев и в каждом слое такое же количество нейронном, что и у EncNet.

Для измерения качества моделей будет считать среднеквадратическое отклонение. Результаты работы алгоритмов показаны на изображении~\eqref{fgr:2}. Качество работы моделей, а также их сложность представлены в таблице~\eqref{tbl:2}.

\begin{figure}[h!]
\begin{center}
    \begin{tabular}{m{.1\textwidth}   m{.1\textwidth}   m{.1\textwidth}   m{.1\textwidth}   m{.1\textwidth}  m{.1\textwidth}  m{.1\textwidth} m{.1\textwidth}}
    \hline   
        \textbf{Left} & \textbf{Right} & \textbf{EncNet1} & \textbf{LinNet1} & \textbf{EncNet2} & \textbf{LinNet2} & \textbf{DumbNet} & \textbf{PLS}\\


        \includegraphics[scale=5]{figures/pred/10.pdf} & 
        \includegraphics[scale=5]{figures/pred/11.pdf} &         \includegraphics[scale=5]{figures/pred/16.pdf} & 
        \includegraphics[scale=5]{figures/pred/17.pdf} & 
        \includegraphics[scale=5]{figures/pred/12.pdf} & 
        \includegraphics[scale=5]{figures/pred/13.pdf} & 
        \includegraphics[scale=5]{figures/pred/14.pdf} & 
        \includegraphics[scale=5]{figures/pred/15.pdf}\\
        
        \includegraphics[scale=5]{figures/pred/20.pdf} & 
        \includegraphics[scale=5]{figures/pred/21.pdf} &         \includegraphics[scale=5]{figures/pred/26.pdf} & 
        \includegraphics[scale=5]{figures/pred/27.pdf} & 
        \includegraphics[scale=5]{figures/pred/22.pdf} & 
        \includegraphics[scale=5]{figures/pred/23.pdf} & 
        \includegraphics[scale=5]{figures/pred/24.pdf} & 
        \includegraphics[scale=5]{figures/pred/25.pdf}\\
        
        \includegraphics[scale=5]{figures/pred/30.pdf} & 
        \includegraphics[scale=5]{figures/pred/31.pdf} &         \includegraphics[scale=5]{figures/pred/36.pdf} & 
        \includegraphics[scale=5]{figures/pred/37.pdf} & 
        \includegraphics[scale=5]{figures/pred/32.pdf} & 
        \includegraphics[scale=5]{figures/pred/33.pdf} & 
        \includegraphics[scale=5]{figures/pred/34.pdf} & 
        \includegraphics[scale=5]{figures/pred/35.pdf}\\
    \hline
    
    \end{tabular}
    \end{center}
	\caption{Восстановление правой части изображения по левой.}
	\label{fgr:2}
\end{figure}

\tikzset{%
  every neuron/.style={
    circle,
    draw,
    minimum size=0.5cm
  },
  neuron missing/.style={
    draw=none, 
    scale=1,
    text height=0.1cm,
    execute at begin node=\color{black}$\vdots$
  },
}

% \begin{tikzpicture}[x=0.7cm, y=0.8cm, >=stealth]

% \foreach \m/\l [count=\y] in {1,2,3,missing,4}
%   \node [every neuron/.try, neuron \m/.try] (input-\m) at (0,2-\y) {};

% \foreach \m [count=\y] in {1,missing,2}
%   \node [every neuron/.try, neuron \m/.try ] (hidden1-\m) at (2,1.5-\y*1.25) {};

% \foreach \m [count=\y] in {1,missing,2}
%   \node [every neuron/.try, neuron \m/.try ] (latent-\m) at (4,1-\y) {};
  
% \foreach \m [count=\y] in {1,missing,2}
%   \node [every neuron/.try, neuron \m/.try ] (hidden3-\m) at (6,1.5-\y*1.25) {};

% \foreach \m/\l [count=\y] in {1,2,3,missing,4}
%   \node [every neuron/.try, neuron \m/.try] (output-\m) at (8,2-\y) {};
  
  
% \foreach \l [count=\i] in {1,2,3,n}
%   \draw [<-] (input-\i) -- ++(-1,0)
%     node [above, midway] {$I_\l$};

% \foreach \l [count=\i] in {1,n}
%   \node [above] at (hidden1-\i.north) {$H_\l$};

% \foreach \l [count=\i] in {1,n}
%   \node [above] at (latent-\i.north) {$L_\l$};
    
% \foreach \l [count=\i] in {1,n}
%   \node [above] at (hidden3-\i.north) {$H_\l$};

% \foreach \l [count=\i] in {1,2,3,n}
%   \draw [->] (output-\i) -- ++(1,0)
%     node [above, midway] {$O_\l$};
    
% \foreach \i in {1,...,4}
%   \foreach \j in {1,...,2}
%     \draw [->] (input-\i) -- (hidden1-\j);

% \foreach \i in {1,...,2}
%   \foreach \j in {1,...,2}
%     \draw [->] (hidden1-\i) -- (latent-\j);
    
% \foreach \i in {1,...,2}
%   \foreach \j in {1,...,2}
%     \draw [->] (latent-\i) -- (hidden3-\j);    

% \foreach \i in {1,...,2}
%   \foreach \j in {1,...,4}
%     \draw [->] (hidden3-\i) -- (output-\j);


% \foreach \l [count=\x from 0] in {Input, Hidden, Latent, Hidden, Output}
%   \node [align=center, above] at (\x*2,1.4) {\l};

% \end{tikzpicture}

% \begin{tikzpicture}[x=0.7cm, y=0.8cm, >=stealth]

% \foreach \m/\l [count=\y] in {1,2,3,missing,4}
%   \node [every neuron/.try, neuron \m/.try] (input-\m) at (0,2-\y) {};

% \foreach \m [count=\y] in {1,missing,2}
%   \node [every neuron/.try, neuron \m/.try ] (hidden1-\m) at (2,1.5-\y*1.25) {};

% \foreach \m [count=\y] in {1,missing,2}
%   \node [every neuron/.try, neuron \m/.try ] (latent-\m) at (4,1-\y) {};
  
% \foreach \m [count=\y] in {1,missing,2}
%   \node [every neuron/.try, neuron \m/.try ] (hidden3-\m) at (6,1.5-\y*1.25) {};

% \foreach \m/\l [count=\y] in {1,2,3,missing,4}
%   \node [every neuron/.try, neuron \m/.try] (output-\m) at (8,2-\y) {};
  
  
% \foreach \l [count=\i] in {1,2,3,n}
%   \draw [<-] (input-\i) -- ++(-1,0)
%     node [above, midway] {$I_\l$};

% \foreach \l [count=\i] in {1,n}
%   \node [above] at (hidden1-\i.north) {$H_\l$};

% \foreach \l [count=\i] in {1,n}
%   \node [above] at (latent-\i.north) {$L_\l$};
    
% \foreach \l [count=\i] in {1,n}
%   \node [above] at (hidden3-\i.north) {$H_\l$};

% \foreach \l [count=\i] in {1,2,3,n}
%   \draw [->] (output-\i) -- ++(1,0)
%     node [above, midway] {$O_\l$};
    
% \foreach \i in {1,...,4}
%   \foreach \j in {1,...,2}
%     \draw [->] (input-\i) -- (hidden1-\j);

% \foreach \i in {1,...,2}
%   \foreach \j in {1,...,2}
%     \draw [->] (hidden1-\i) -- (latent-\j);
    
% \foreach \i in {1,...,2}
%   \foreach \j in {1,...,2}
%     \draw [->] (latent-\i) -- (hidden3-\j);    

% \foreach \i in {1,...,2}
%   \foreach \j in {1,...,4}
%     \draw [->] (hidden3-\i) -- (output-\j);


% \end{tikzpicture}




%%% если имеется doi цитируемого источника, необходимо его указать, см. пример в \bibitem{article}
%%% DOI публикации, зарегистрированной в системе Crossref, можно получить по адресу http://www.crossref.org/guestquery/

\bibliographystyle{unsrt}
\bibliography{links}

\end{document}


